<html>
  <head />
  <body>
    <p>For this Tennis project, I implemented DDPPG algorithm for training the Reacher env.</p>
    <p>It has been trained by running 5837 episodes, and having average of 0.52 in the end.</p>
    <p>For the chosen hyper parameters, I used:
      n_episodes=10000,        # Number of episodes
      max_t=1000-,             # Number of steps per episode
      BUFFER_SIZE = int(1e6)  # replay buffer size
      BATCH_SIZE = 128        # minibatch size
      GAMMA = 0.99            # discount factor
      TAU = 1e-3              # for soft update of target parameters                                                                                                                 LR_ACTOR = 1e-4         # learning rate of the actor
      LR_CRITIC = 1e-4        # learning rate of the critic
      WEIGHT_DECAY = 0        # L2 weight decay
    </p>
    <p> Model architecture:
      The actor neural network consists of three layers:
      - A fully connected layer with input of state size and output of 512 units
      - A fully connected layer with input of 512 units and output of 256 units
      - A fully connected layer with input of 256 units and output of the action size

      The critic neural network consists of three layers:
      - A fully connected layer with input of state size and output of 96 units
      - A fully connected layer with input of 96 units and action size and output of 96 units
      - A fully connected layer with input of 96 units and output of 1
    </p>
    <p> Furthur improvements:
      We can add Batch normalization to the existing implementation. We can also try out different algorithms like PPO, TRPO or TNPG.
      We can also improve the overall hyperparamter search with some grid search opportunities, and also potentially searching for a better neural network with AutoML search.
    </p>
    <img src="chart.png" />
  </body>
</html>
